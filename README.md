# penuX ‚úÖ

Multimodal pipeline and analysis utilities for chest x-ray + clinical data (Normal / Bacterial / Viral). This repository contains data preparation, model training/evaluation, analysis notebooks, and utilities including correlation-matrix generation and per-epoch correlation saving during training.

---

## Table of contents

- [Features](#features)
- [Requirements](#requirements)
- [Quick start](#quick-start)
- [Correlation matrices (new)](#correlation-matrices-new)
  - [Standalone script](#standalone-script)
  - [Notebook example](#notebook-example)
  - [Per-epoch correlation saving during training](#per-epoch-correlation-saving-during-training)
- [Training & evaluation](#training--evaluation)
- [Tests](#tests)
- [Notebooks](#notebooks)
- [Development & contribution](#development--contribution)
- [License](#license)

---

## Features ‚ú®

- End-to-end multimodal (image + clinical) training scripts and examples
- Auto-generation / optional download of synthetic clinical.csv aligned to Kaggle images
- Utilities to compute and save Pearson and Spearman correlation matrices and heatmaps
- Optional per-epoch correlation capture (CSV + heatmap) during training
- Example notebook demonstrating correlation combinations and visualizations
- Unit tests for the correlation tooling

---

## Requirements ‚öôÔ∏è

- Python 3.8+ (the dev container uses Ubuntu 24.04 with Python 3.11)
- Recommended packages: numpy, scipy, pandas, seaborn, matplotlib, scikit-learn, tensorflow (as used by the training scripts)

Install (minimal) via pip:

```bash
python -m pip install -r requirements.txt  # if a requirements file exists
# or install directly
python -m pip install numpy pandas seaborn matplotlib scikit-learn pytest
```

Note: TensorFlow and other heavy deps are only necessary for training and evaluation.

---

## Quick start ‚ñ∂Ô∏è

1. Prepare dataset (Kaggle Mooney chest x-ray) and clinical CSV (auto-download or auto-generate):

   - The scripts `model.py`, `model2.py`, `model3.py`, and `m.py` include helpers to prepare or auto-generate clinical data and dataset folders.

2. Compute correlation matrices for a CSV file (see next section) or run training.

---

## Correlation matrices (new) üîç

### Standalone script

A reusable script computes Pearson and Spearman correlation matrices for numeric columns and saves both CSVs and heatmap PNGs.

Usage:

```bash
python -m scripts.compute_correlations --input clinical.csv --outdir outputs/correlations --prefix clinical
```

Options:
- `--input` / `-i`: input CSV (default: `clinical.csv`)
- `--outdir` / `-o`: output folder (default: `outputs/correlations`)
- `--prefix` / `-p`: filename prefix (default: `clinical`)
- `--columns` / `-c`: comma-separated list of columns to include (default: all numeric columns)

Outputs:
- `<outdir>/<prefix>_pearson.csv` ‚Äî Pearson correlation coefficients (CSV)
- `<outdir>/<prefix>_spearman.csv` ‚Äî Spearman correlation coefficients (CSV)
- `<outdir>/<prefix>_pearson.png` ‚Äî Pearson heatmap (PNG)
- `<outdir>/<prefix>_spearman.png` ‚Äî Spearman heatmap (PNG)

Tip: Use `--columns temperature_c,wbc,spo2,age` to restrict to specific features.

### Notebook example

A short example notebook is available at `notebooks/correlations_example.ipynb` demonstrating programmatic use of the script and inline display of heatmaps.

### Per-epoch correlation saving during training

... (see above for usage; defaults and auto-downsampling rules apply)

### Predicting pathogens from vitals üß™

You can predict pathogen / class probabilities from clinical vitals using the new `scripts/predict_pathogen.py` utility.

Single prediction example (prints and can save CSV/JSON):

```bash
python -m scripts.predict_pathogen --single --task specpath --temperature_c 37.1 --wbc 7000 --spo2 96 --age 60 --format csv --output pred.csv
```

Batch prediction from CSV (input must include `temperature_c,wbc,spo2,age`):

```bash
python -m scripts.predict_pathogen --input patients.csv --format json --output preds.json --task specpath
```

Notes:
- The script requires trained clinical models `clin_encoder.keras`, `clin_head.keras` and `clin_scaler.npz` to exist in the repo root (these are produced during training).
- For `specpath`, class names are loaded from `pathogen_vocab.json` (generated by the feature-building step).
- Output includes class probability columns; use `--top K` for a top-K summary per row.

The training pipeline in `m.py` supports saving per-epoch correlation matrices of learned embeddings (image vs. clinical) so you can track how correlations evolve during training.

- CLI flags added to `m.py`:
  - `--corr_every`: Save correlation matrices every N epochs (integer). Set `0` to disable. Default: `1` (every epoch).
  - `--corr_dir`: Output directory for correlation files. Default: `corrs`.

Example (enable per-epoch saving):

```bash
python m.py --task specpath --epochs 20 --steps_per_epoch 200 --corr_every 1 --corr_dir corrs
```

What it saves (per epoch):
- `corrs/<epoch>_pearson.csv`
- `corrs/<epoch>_spearman.csv`
- `corrs/<epoch>_pearson.png`
- `corrs/<epoch>_spearman.png`

Note: for very large runs we auto-adjust the sampling frequency to avoid excessive files. If you leave `--corr_every` at the default (1) and run with `--epochs >= 1000`, the script will automatically set `--corr_every` to `10` and print an informational message. You can always override with `--corr_every <N>`.

You can also call `train_dual(..., corr_every=N, corr_out_dir=Path('corrs'))` from Python when running programmatically.

---

## Training & evaluation üèÉ‚Äç‚ôÇÔ∏è

High-level training flow is provided in `model.py`, `model2.py`, `model3.py`, and the dual-correlation training operations are in `m.py` (including `train_dual`). See each file's docstrings and `--help` for CLI options.

Common example:

```bash
# Train using the Mooney dataset (auto-prep if necessary)
python model.py
# or run the dual-correlation training
python m.py --task specpath --epochs 20 --steps_per_epoch 200 --corr_every 1 --corr_dir corrs
```

After training, evaluation artifacts are created (model files, confusion matrix image, AUC printed to console).

---

## Tests ‚úÖ

Unit tests for correlation utilities are in `tests/test_compute_correlations.py`.

Run tests with pytest:

```bash
pytest -q
```

---

## Notebooks üìì

- `notebooks/correlations_example.ipynb` ‚Äî demonstrates computing correlations, showing heatmaps, and programmatic usage of the script.

---

## Development & contribution üë©‚Äçüíª

- Use the `tests/` folder and `pytest` to add unit tests for new code.
- Keep code style consistent and add docstrings for public functions.
- If you add dataset download or heavy dependencies, document them in the README and in code comments.

---

## License

No repository-level LICENSE file is present. If you plan to distribute this code, add a LICENSE (e.g., MIT) and document any dataset license constraints separately.

---

If you want, I can:
- Add example outputs (sample correlation images) under `docs/` or `examples/` ‚úÖ
- Add a small CLI example to run correlations as part of CI (smoke test) ‚úÖ
- Add a brief troubleshooting section for common issues (missing CSVs, missing packages) ‚úÖ

If you'd like any of those, tell me which and I'll add them.